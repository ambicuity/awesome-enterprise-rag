# Enterprise RAG: A Practical Guide from the Trenches

> **Author:** Ritesh Rana  
> **Email:** riteshrana36@gmail.com  
> **Website:** [riteshrana.engineer](https://riteshrana.engineer)

A comprehensive guide to building production-ready Retrieval-Augmented Generation (RAG) systems for mid-size enterprise companies based on real-world experience with 10+ clients in regulated industries.

---

## ðŸ“š Table of Contents

1. [Introduction](#introduction)
2. [Context & Background](#context--background)
3. [Core Challenges](#core-challenges)
4. [Document Quality Detection](#document-quality-detection)
5. [Chunking Strategies](#chunking-strategies)
6. [Metadata Architecture](#metadata-architecture)
7. [Hybrid Search Approaches](#hybrid-search-approaches)
8. [Model Selection & Deployment](#model-selection--deployment)
9. [Table Processing](#table-processing)
10. [Production Infrastructure](#production-infrastructure)
11. [Key Lessons Learned](#key-lessons-learned)
12. [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)
13. [Resources & References](#resources--references)

---

## Introduction

This repository contains learnings from building RAG systems for mid-size enterprise companies (100-1000 employees) in regulated spaces over the past year. If you're expecting another basic tutorial, this isn't it. This is the real-world, battle-tested knowledge from working with pharma companies, banks, law firms, and consulting shops.

**What makes this different:**
- Real enterprise document challenges (not clean datasets)
- Production infrastructure considerations
- Regulatory compliance requirements
- Cost and performance optimization strategies
- Actual failure modes and solutions

---

## Context & Background

### The Enterprise Reality

Most enterprise companies have:
- **10K-50K+ documents** sitting in SharePoint or legacy document management systems
- **Decades of business documents** that need to become searchable
- **Not clean datasets** - just real-world, messy documents
- **Mixed quality** - documents from 1995 scanned typewritten pages mixed with modern 500+ page reports

### Industries Covered

This guide draws from experience with:
- **Pharmaceutical Companies**: Research papers, clinical trials, regulatory documents
- **Financial Institutions**: Financial reports, compliance documents, risk assessments
- **Law Firms**: Legal briefs, case files, contracts
- **Consulting Firms**: Client reports, proposals, research documents

---

## Core Challenges

### Why Enterprise RAG is Different

Enterprise RAG is fundamentally different from tutorial projects:

1. **Document Quality Varies Wildly**: From perfect PDFs to scanned handwritten notes
2. **Domain-Specific Context**: Acronyms, terminology, and references are highly specialized
3. **Regulatory Constraints**: Data cannot leave client infrastructure
4. **Scale & Performance**: Thousands of daily queries across massive document sets
5. **Complex Structures**: Tables, charts, cross-references, multi-section documents

### Common Failure Modes

- **Acronym Confusion**: Same acronym, different meanings across domains
- **Precision Queries**: "What was the exact dosage in Table 3?" vs conceptual questions
- **Cross-References**: Documents referencing other documents
- **Table Data**: Critical information locked in complex tables
- **Context Loss**: Chunking destroys document structure and relationships

---

## Document Quality Detection

### The Problem Nobody Talks About

**Reality Check**: Most tutorials assume your PDFs are perfect. Enterprise documents are absolute garbage.

### The Revelation

Document quality varies so dramatically that applying the same processing pipeline to all documents returns complete nonsense. Different quality levels require different processing strategies.

### Quality Scoring System

Build a scoring system that evaluates:
- **Text Extraction Quality**: How well can text be extracted?
- **OCR Artifacts**: Presence of OCR errors and artifacts
- **Formatting Consistency**: Structural integrity of the document

### Processing Pipeline Routing

Based on quality scores, route documents to appropriate pipelines:

#### Clean PDFs (Score: 8-10)
- Text extraction works perfectly
- Apply full hierarchical processing
- Preserve all structural elements

#### Decent Documents (Score: 5-7)
- Some OCR artifacts present
- Basic chunking with cleanup steps
- Focus on content over structure

#### Garbage Documents (Score: 0-4)
- Scanned handwritten notes, heavily degraded PDFs
- Simple fixed chunks
- Add manual review flags
- Consider alternative extraction methods

### Implementation Impact

**Single biggest fix**: Document quality detection fixed more retrieval issues than any embedding model upgrade.

```
Quality Assessment â†’ Pipeline Routing â†’ Optimized Processing
```

---

## Chunking Strategies

### Why Fixed-Size Chunking is Wrong

**Tutorial Approach**: "Just chunk everything into 512 tokens with overlap!"

**Reality**: Documents have structure. Ignoring it leads to:
- Chunks that cut off mid-sentence
- Unrelated concepts combined
- Loss of hierarchical context
- Poor retrieval accuracy

### Hierarchical Chunking Architecture

Implement multi-level chunking that preserves document structure:

#### Level 1: Document Level
- Title
- Authors
- Date
- Document Type
- Summary/Abstract

#### Level 2: Section Level
- Abstract
- Introduction
- Methods/Methodology
- Results
- Discussion
- Conclusion

#### Level 3: Paragraph Level
- 200-400 tokens per chunk
- Preserve paragraph boundaries
- Maintain context

#### Level 4: Sentence Level
- For precision queries
- Exact data extraction
- Table and figure references

### Query-Driven Retrieval Strategy

**Key Insight**: Query complexity should determine retrieval level.

#### Broad Questions â†’ Paragraph Level
- "What are the benefits of Drug X?"
- "Explain the financial performance in 2023"
- "Summarize the clinical trial results"

#### Precise Questions â†’ Sentence Level
- "What was the exact dosage in Table 3?"
- "What is the specific FDA approval date?"
- "List the exact contraindications"

### Precision Mode Triggers

Use simple keyword detection to trigger precision mode:

**Trigger Words** (20-30 terms):
- "exact"
- "specific"
- "table"
- "figure"
- "dosage"
- "date"
- "number"
- "value"

**Automatic Drill-Down**: If confidence is low at paragraph level, automatically drill down to sentence level for more precision.

---

## Metadata Architecture

### Why Metadata Matters Most

**40% of development time, highest ROI of any component built.**

Most people treat metadata as an afterthought. But enterprise queries are crazy contextual.

### Domain-Specific Schemas

#### Pharmaceutical Documents

```
- Document Type: [research_paper, regulatory_doc, clinical_trial, etc.]
- Drug Classifications: [oncology, cardiology, neurology, etc.]
- Patient Demographics: [pediatric, adult, geriatric]
- Regulatory Categories: [FDA, EMA, ICH, etc.]
- Therapeutic Areas: [cardiology, oncology, immunology, etc.]
- Study Phase: [phase_1, phase_2, phase_3, phase_4]
- Publication Date: [YYYY-MM-DD]
```

#### Financial Documents

```
- Document Type: [10-K, 10-Q, earnings_report, analyst_report, etc.]
- Time Periods: [Q1_2023, FY_2022, etc.]
- Financial Metrics: [revenue, EBITDA, net_income, etc.]
- Business Segments: [retail, wholesale, services, etc.]
- Geographic Regions: [north_america, europe, asia_pacific, etc.]
- Report Type: [annual, quarterly, monthly]
```

#### Legal Documents

```
- Document Type: [contract, brief, memo, opinion, etc.]
- Practice Area: [corporate, litigation, IP, etc.]
- Jurisdiction: [federal, state, international]
- Case Status: [active, closed, pending]
- Document Date: [YYYY-MM-DD]
```

### Metadata Extraction Strategy

**Avoid LLMs for Metadata Extraction**: They're inconsistent as hell.

#### Use Simple Keyword Matching

**Approach**:
1. Start with 100-200 core terms per domain
2. Query contains "FDA"? â†’ Filter for `regulatory_category: "FDA"`
3. Mentions "pediatric"? â†’ Apply `patient_population: "pediatric"` filter
4. Build with domain experts who know the terminology

**Benefits**:
- Consistent results
- Debuggable
- Fast execution
- No API costs

#### When to Use Small Models

For **classification tasks** (not extraction):
- Route document to correct pipeline
- Determine document type (research paper vs financial report)
- Binary classification decisions

Use 7B-13B models for these tasks.

#### Regex and Pattern Matching

For **specific data extraction**:
- Publication dates
- Drug names
- Financial metrics
- Reference numbers

Use deterministic approaches (regex, pattern matching).

---

## Hybrid Search Approaches

### When Semantic Search Fails

**Admission**: Pure semantic search fails way more than people admit.

In specialized domains like pharma and legal: **15-20% failure rate** (not the assumed 5%)

### Main Failure Modes

#### 1. Acronym Confusion

**Example**: 
- "CAR" = "Chimeric Antigen Receptor" (oncology)
- "CAR" = "Computer Aided Radiology" (imaging)

Same embedding, completely different meanings.

**Solution**: Context-aware acronym expansion using domain-specific acronym databases.

#### 2. Precise Technical Queries

**Example**: "What was the exact dosage in Table 3?"

Semantic search finds conceptually similar content but misses the specific table reference.

**Solution**: Keyword triggers switch to rule-based retrieval for specific data points.

#### 3. Cross-Reference Chains

Documents reference other documents constantly. Drug A study references Drug B interaction data.

Semantic search misses these relationship networks completely.

**Solution**: Build graph layer to track document relationships.

### Hybrid Architecture

```
User Query
    â†“
Semantic Search (Vector Similarity)
    â†“
Graph Relationship Check
    â†“
Related Documents with Better Answers?
    â†“
Re-rank and Return Results
```

### Implementation Steps

#### Step 1: Semantic Search
- Use embeddings for initial retrieval
- Get top K candidates (K=10-20)

#### Step 2: Graph Layer
- Track document relationships during processing
- Check if retrieved docs have related documents

#### Step 3: Precision Fallback
- Detect precision queries via keywords
- Switch to rule-based retrieval
- Target specific tables, figures, data points

#### Step 4: Acronym Handling
- Detect acronyms in query
- Expand using domain-specific database
- Search with expanded terms

---

## Model Selection & Deployment

### Why Open Source Models?

Most assume GPT-4o or o3-mini are always better. But enterprise clients have constraints:

#### Cost
- API costs explode with 50K+ documents
- Thousands of daily queries
- Long-term operational expenses

#### Data Sovereignty
- Pharma and finance cannot send sensitive data to external APIs
- Regulatory requirements for data residency
- Zero data retention not sufficient for compliance

#### Domain Terminology
- General models hallucinate on specialized terms
- Not trained on domain-specific vocabulary
- Inconsistent with technical accuracy

### Qwen QWQ-32B Solution

After testing multiple models, Qwen QWQ-32B emerged as the winner:

**Advantages**:
- **85% cheaper** than GPT-4o for high-volume processing
- **On-premise deployment** - everything stays on client infrastructure
- **Fine-tunable** on medical/financial terminology
- **Consistent response times** without API rate limits
- **Reliable performance** after domain-specific tuning

### Fine-Tuning Approach

#### Straightforward Supervised Training

1. Create domain Q&A pairs:
   - "What are contraindications for Drug X?" â†’ FDA guideline answers
   - "What was Q3 2023 revenue?" â†’ Exact financial data
   - "What are the study inclusion criteria?" â†’ Clinical trial protocol

2. Basic supervised fine-tuning (not complex methods like RAFT)

3. **Key**: Having clean training data

**Result**: Consistent, accurate responses for domain-specific queries.

### Cost Comparison

#### GPT-4o Costs
- **Input**: $2.50 per 1M tokens
- **Output**: $10.00 per 1M tokens

**Example Workload** (50K documents, 10K queries/month):
- Initial embedding: ~$125
- Monthly queries: ~$100
- **Total**: $200-300+/month (scales rapidly)

#### Qwen QWQ-32B Costs
- **Input**: $0.15-0.50 per 1M tokens (varies by provider)
- **Output**: $0.45-1.50 per 1M tokens
- **Groq**: $0.29/$0.39 per million tokens

**Same Workload**:
- Initial embedding: ~$15-25
- Monthly queries: ~$15-20
- **Total**: $30-50/month

**Savings**: 85% reduction at enterprise scale

### Hardware Requirements

#### Capital Cost Considerations
- **Consumer GPUs**: Won't work for production
- **Recommended**: A100 GPUs (~$17K each)
- **Reality**: Most enterprise clients already have GPUs from other projects

#### Deployment Stack
- **Inference Servers**: Ollama or vLLM
- **Quantization**: 4-bit quantization for Qwen QWQ-32B
- **Memory**: 24GB VRAM for quantized model (single RTX 4090)
- **Production**: A100s better for concurrent users

#### Multi-Model Architecture

Deploy 2-3 models for optimal performance:

1. **Main Generation Model**: Qwen 32B for complex queries
2. **Lightweight Model**: 7B-13B for metadata extraction/classification
3. **Specialized Embedding Model**: Domain-optimized embeddings

### Performance Metrics

#### Inference Rate
- **Single User**: ~40+ tokens/sec (varies by hardware)
- **10 Concurrent Users** (H100, optimized context): ~40+ tokens/sec
- **Context Impact**: Less context = more concurrent users

#### Deployment Simplicity
- On-premise deployment straightforward
- Use existing GPU infrastructure
- Ollama or vLLM for serving

---

## Table Processing

### The Hidden Nightmare

**Reality**: Enterprise documents are full of complex tables.

- Financial models
- Clinical trial data
- Compliance matrices
- Regulatory submissions

**Standard RAG Problems**:
- Ignores tables completely
- Extracts tables as unstructured text
- Loses all relationships
- Misses critical information

### Why Tables Matter

**Tables contain the most critical information**:
- Financial analysts need exact numbers from specific quarters
- Researchers need dosage info from clinical tables
- Compliance officers need regulatory data from matrices

**If you can't handle tabular data, you're missing half the value.**

### Table Processing Pipeline

#### Step 1: Table Detection
- Use heuristics for detection
- Spacing patterns
- Grid structures
- Visual markers

#### Step 2: Structure Preservation

**Simple Tables**:
- Convert to CSV format
- Maintain row/column relationships

**Complex Tables**:
- Preserve hierarchical relationships in metadata
- Track merged cells
- Maintain nested structures

#### Step 3: Dual Embedding Strategy

Embed both:
1. **Structured Data**: Raw table content
2. **Semantic Description**: What the table represents

**Example**:
```
Structured: "Q1 2023 | Revenue | $1.2M | 15% YoY"
Semantic: "Quarterly revenue performance showing growth"
```

#### Step 4: Relationship Tracking

For financial tables:
- Link summary tables to detailed breakdowns
- Track hierarchical relationships
- Enable drill-down queries

### Why Not PDF to HTML?

**Attempted but problematic**:

#### Conversion Issues
- Mangles table structures
- Merged cells break
- Nested tables corrupted
- Spatial relationships lost

#### HTML Quality
- Auto-converted HTML is messy
- Doesn't match model training data
- Clean web HTML â‰  PDF-converted HTML

#### Better Alternative: VLMs
- Visual Language Models work directly on PDF page images
- More reliable than HTML intermediates
- Better at understanding complex layouts

**Verdict**: For clean, simple documents, HTML might work. For complex enterprise documents, VLMs are superior.

---

## Production Infrastructure

### Reality Check

Tutorials assume:
- Unlimited resources
- Perfect uptime
- No concurrency issues

Production means:
- Concurrent users
- GPU memory management
- Consistent response times
- Uptime guarantees

### Resource Management

#### GPU Infrastructure

**Common Scenario**: 
- Clients already have GPU infrastructure
- Unused compute from other projects
- Makes on-premise deployment easier

**Typical Setup**:
- 2-3 models deployed simultaneously
- Quantized versions when possible
- Shared GPU resources

#### Preventing Resource Contention

**Biggest Challenge**: Multiple users hitting the system simultaneously

**Solutions**:
- Use semaphores to limit concurrent model calls
- Proper queue management
- Load balancing across GPUs
- Request throttling

### Deployment Architecture

#### Model Configuration

**Main Generation Model**:
- Qwen QWQ-32B (quantized to 4-bit)
- 24GB VRAM requirement
- Single RTX 4090 or A100

**Lightweight Model**:
- 7B-13B for classification
- Minimal VRAM
- Fast inference

**Embedding Model**:
- Domain-optimized
- Batch processing
- Separate from generation

#### Context Management

**Critical Factor**: Context length impacts concurrency

**Strategy**:
- Cap context per request
- Optimize for typical queries
- Less context = more concurrent users
- Monitor and adjust based on load

### Reliability Over Features

**Client Priorities**:
1. **Uptime**: System availability
2. **Response Time**: Consistent performance
3. **Accuracy**: Correct results
4. **Features**: Nice-to-haves

**Key Insight**: Resource management and uptime matter more than model sophistication.

### Monitoring & Maintenance

#### Essential Metrics
- Request latency
- GPU utilization
- Queue depth
- Error rates
- Cache hit rates

#### Operational Procedures
- Automated health checks
- Graceful degradation
- Failover strategies
- Regular model updates

---

## Key Lessons Learned

### 1. Document Quality Detection First

**Priority #1**: You cannot process all enterprise docs the same way.

**Action**: Build quality assessment before anything else.

**Impact**: Single biggest improvement to retrieval accuracy.

### 2. Metadata > Embeddings

**Reality**: Poor metadata means poor retrieval regardless of vector quality.

**Action**: Spend the time on domain-specific schemas.

**ROI**: 40% of dev time, highest return on investment.

### 3. Hybrid Retrieval is Mandatory

**Reality**: Pure semantic search fails too often in specialized domains.

**Action**: Need rule-based fallbacks and document relationship mapping.

**Coverage**: Reduces failure rate from 15-20% to <5%.

### 4. Tables are Critical

**Reality**: Critical information locked in tabular data.

**Action**: Build dedicated table processing pipeline.

**Value**: Unlocks half of enterprise document value.

### 5. Infrastructure Determines Success

**Reality**: Clients care more about reliability than fancy features.

**Action**: Resource management and uptime are paramount.

**Focus**: Engineering over ML sophistication.

### 6. Smaller Models Have Their Place

**Reality**: Don't need 32B parameters for everything.

**Action**: 
- 7B-13B for classification and routing
- 32B for complex reasoning and synthesis

**Efficiency**: Right-sized models for each task.

### 7. Acronyms Are Everywhere

**Reality**: Every department creates their own shorthand.

**Action**: Build domain-specific acronym dictionaries.

**Acceptance**: There will always be new ones.

### 8. Full Document Processing for Small Sets

**Reality**: For small, high-quality datasets, chunking might not be necessary.

**Action**: 
- Documents <10-20 pages: consider full-document processing
- Avoids context loss
- No retrieval failures

**Tradeoff**: Slower processing, higher compute, better accuracy.

### 9. Simple Approaches Win

**Reality**: Complex scoring algorithms are hard to debug.

**Action**: 
- Keyword matching over LLM extraction
- Regex over AI parsing
- Simple confidence thresholds

**Benefit**: Reliable and debuggable systems.

### 10. The Real Talk

**Enterprise RAG is way more engineering than ML.**

Most failures aren't from bad models - they're from:
- Underestimating document processing challenges
- Ignoring metadata complexity
- Not planning for production infrastructure needs

**The Demand**: Every company with substantial document repositories needs these systems.

**The Reality**: Most have no idea how complex it gets with real-world documents.

**The ROI**: When it works, teams cut document search from hours to minutes.

---

## Frequently Asked Questions (FAQ)

### General Questions

#### Q: What makes enterprise RAG different from tutorial projects?

**A**: Enterprise RAG deals with real-world messy documents (decades old scanned papers, complex tables, domain-specific terminology), regulatory constraints (data cannot leave infrastructure), and production requirements (thousands of concurrent users, uptime guarantees). Tutorials use clean datasets and ignore these challenges.

#### Q: What industries benefit most from enterprise RAG?

**A**: Regulated industries with large document repositories:
- **Pharmaceutical**: Research papers, clinical trials, regulatory documents
- **Financial**: Financial reports, compliance documents, risk assessments  
- **Legal**: Legal briefs, case files, contracts
- **Healthcare**: Patient records, medical research, clinical guidelines
- **Consulting**: Client reports, proposals, research documents

#### Q: What's the typical scale of enterprise RAG deployments?

**A**: Mid-size enterprises (100-1000 employees) typically have:
- 10K-50K+ documents
- Thousands of daily queries
- Multiple concurrent users
- Documents ranging from 1-500+ pages

### Document Processing

#### Q: How do I handle documents of varying quality?

**A**: Implement a quality scoring system:
1. **Score documents** based on text extraction quality, OCR artifacts, formatting consistency
2. **Route to appropriate pipelines**: Clean PDFs get full hierarchical processing, degraded documents get simpler processing with manual review flags
3. This single change fixes more issues than any embedding model upgrade

#### Q: Should I use fixed-size chunking?

**A**: No. Fixed-size chunking (e.g., "512 tokens with overlap") ignores document structure and leads to poor retrieval. Use hierarchical chunking:
- **Document level**: Title, authors, metadata
- **Section level**: Abstract, Methods, Results
- **Paragraph level**: 200-400 tokens
- **Sentence level**: For precision queries

Match retrieval level to query complexity.

#### Q: How do I handle tables in enterprise documents?

**A**: Build a dedicated table processing pipeline:
1. **Detect tables** using heuristics (spacing patterns, grid structures)
2. **Simple tables**: Convert to CSV
3. **Complex tables**: Preserve hierarchical relationships in metadata
4. **Dual embedding**: Embed both structured data AND semantic description
5. **Track relationships**: Link summary tables to detailed breakdowns

Tables contain critical information - if you can't handle them, you're missing half the value.

#### Q: What about PDF to HTML conversion for tables?

**A**: Generally not recommended for complex enterprise documents:
- Conversion mangles table structures (merged cells, nested tables)
- Auto-converted HTML doesn't match clean web HTML
- Visual Language Models (VLMs) working directly on PDF images are more reliable
- HTML approach might work for very clean, simple documents only

### Metadata & Search

#### Q: How important is metadata compared to embeddings?

**A**: Metadata is MORE important than embeddings. Poor metadata means poor retrieval regardless of vector quality. Spend 40% of development time on domain-specific metadata schemas - it has the highest ROI.

#### Q: Should I use LLMs for metadata extraction?

**A**: No, for specific data extraction (dates, names, categories):
- **Don't use LLMs**: They're inconsistent and unreliable
- **Use keyword matching and regex**: Consistent, debuggable, fast
- **Small models OK for classification**: 7B-13B models work for document type routing
- **Rule**: Use the simplest approach that works reliably

#### Q: How do I build metadata schemas?

**A**: Work with domain experts:
1. Start with 100-200 core terms per domain
2. Build domain-specific categories (for pharma: drug classifications, patient demographics, regulatory categories)
3. Use simple keyword matching (query contains "FDA"? Filter for regulatory_category: "FDA")
4. Expand based on queries that don't match well
5. Keep it simple and debuggable

#### Q: When does pure semantic search fail?

**A**: In specialized domains, semantic search fails 15-20% of the time:
- **Acronym confusion**: Same acronym, different meanings (CAR = Chimeric Antigen Receptor vs Computer Aided Radiology)
- **Precision queries**: "What was exact dosage in Table 3?" finds similar content but misses specific reference
- **Cross-references**: Misses document relationship networks

Solution: Hybrid approach with graph layer and rule-based fallbacks.

#### Q: How do I implement hybrid search?

**A**: Combine multiple approaches:
1. **Semantic search**: Vector similarity for initial retrieval
2. **Graph layer**: Track document relationships, check for related documents with better answers
3. **Precision fallback**: Keyword triggers switch to rule-based retrieval for specific data
4. **Acronym handling**: Context-aware expansion using domain databases

### Model Selection

#### Q: Should I use GPT-4o or open source models?

**A**: Open source models (especially Qwen QWQ-32B) for enterprise:

**Open Source Advantages**:
- 85% cheaper than GPT-4o at scale
- Data stays on-premise (regulatory requirement)
- Fine-tunable for domain terminology
- Consistent performance without API limits

**GPT-4o**: $200-300+/month for moderate usage  
**Qwen QWQ-32B**: $30-50/month for same workload

#### Q: What are the hardware requirements?

**A**: For production deployment:
- **Don't use consumer GPUs** - insufficient for production
- **Recommended**: A100 GPUs (~$17K each, but clients often have existing infrastructure)
- **Qwen QWQ-32B quantized** (4-bit): 24GB VRAM (single RTX 4090 works, A100 better for concurrent users)
- **Deploy 2-3 models**: Main generation (32B), lightweight classification (7B-13B), embedding model

#### Q: How fast is inference with Qwen 32B?

**A**: Depends on hardware and concurrency:
- **Single user**: Variable based on hardware
- **10 concurrent users** (H100 with optimized context): ~40+ tokens/sec
- **Key factor**: Context length - less context allows more concurrent users
- Response times consistent without API rate limits

#### Q: Can smaller models handle enterprise tasks?

**A**: Yes, for specific tasks:
- **7B-13B models**: Document classification, routing, basic categorization
- **32B models**: Complex reasoning, domain-specific synthesis, multi-document analysis
- **Right-size models**: Use smallest model that reliably handles each task
- **Failure point**: Small models fail at "find cardiovascular safety signals across 50 clinical trials"

#### Q: How do I fine-tune models for enterprise domains?

**A**: Straightforward supervised training:
1. **Create domain Q&A pairs**: "What are contraindications for Drug X?" paired with FDA guideline answers
2. **Use basic supervised fine-tuning**: Complex methods like RAFT not necessary
3. **Key success factor**: Clean training data quality
4. **Result**: Consistent, accurate domain-specific responses

### Production & Infrastructure

#### Q: How do I handle concurrent users?

**A**: Proper resource management:
- **Use semaphores** to limit concurrent model calls
- **Queue management**: Proper request queuing
- **Context optimization**: Less context per request = more concurrent users
- **Monitor**: Track GPU utilization, queue depth, latency

#### Q: What deployment tools should I use?

**A**: Simple, proven tools:
- **Ollama or vLLM**: For model serving
- **On-premise**: Everything on client infrastructure (regulatory requirement)
- **No cloud APIs**: Even with zero retention, compliance won't allow it
- **Deployment**: Straightforward with existing GPU infrastructure

#### Q: What metrics matter in production?

**A**: Clients prioritize:
1. **Uptime**: System availability > features
2. **Response time**: Consistent performance
3. **Accuracy**: Correct results
4. **Resource efficiency**: Cost management

Infrastructure and reliability matter more than model sophistication.

#### Q: How do I handle documents vs indexing for small datasets?

**A**: For small, high-quality datasets:
- **Documents <10-20 pages**: Consider reading full documents during generation
- **Advantages**: No context loss from chunking, no retrieval failures, more coherent answers
- **Tradeoffs**: Slower processing, higher compute costs
- **When it works**: Policy documents, technical manuals, research papers where full context matters
- **When it doesn't**: Large collections where you need targeted retrieval efficiency

### Cost & ROI

#### Q: What are realistic costs for enterprise RAG?

**A**: Depends on scale and model choice:

**GPT-4o** (50K docs, 10K queries/month):
- Initial: ~$125
- Monthly: ~$100
- Total: $200-300+/month (scales rapidly)

**Qwen QWQ-32B** (same workload):
- Initial: ~$15-25  
- Monthly: ~$15-20
- Total: $30-50/month

**Capital costs** (on-premise):
- A100 GPUs: ~$17K each
- Often clients have existing GPU infrastructure

#### Q: What's the ROI of enterprise RAG?

**A**: Significant time savings:
- **Before**: Document search takes hours
- **After**: Document search takes minutes
- **Impact**: Teams become dramatically more efficient
- **Demand**: Every company with substantial document repositories needs these systems

#### Q: Is the complexity worth it?

**A**: Yes, but be realistic:
- **Enterprise RAG is 90% engineering, 10% ML**
- Most failures are from underestimating document processing challenges
- Edge cases with enterprise documents are frustrating
- When it works, the impact is substantial
- The demand is "honestly crazy right now"

### Troubleshooting

#### Q: How do I handle acronyms?

**A**: Build domain-specific dictionaries:
- Every department/domain has its own acronym shorthand
- Build domain-specific acronym databases for context-aware expansion
- Accept that there will always be new ones (e.g., "EBITDARM" in real estate = EBITDA + rent + management fees)
- Need separate approaches for each industry vs trying to build universal solution

#### Q: How do I determine if confidence is low?

**A**: Simple thresholds:
- **Similarity scores**: If best results <0.7 similarity, trigger precision mode
- **Score distribution**: If top results from completely different document sections, go deeper
- **Keyword detection**: Exact string matching with 20-30 trigger words ("exact", "specific", "table", "dosage")
- **Automatic drill-down**: Low confidence at paragraph level â†’ switch to sentence level

#### Q: How do I handle table references in queries?

**A**: Pattern matching:
- **Regex patterns**: Catch "Table 3", "Figure 2", etc. in queries
- **Preprocessing**: Tag chunks containing tables during document processing
- **Filtering**: When table reference detected, filter specifically for tabular content
- **Precision mode**: Table references trigger sentence-level retrieval

### Best Practices

#### Q: What's the most important lesson learned?

**A**: Document quality detection is #1 priority. You cannot process all enterprise documents the same way. Build quality assessment before anything else - it has more impact than any embedding model upgrade.

#### Q: What's the right approach to chunking?

**A**: Match retrieval strategy to query type:
- **Broad conceptual questions**: Paragraph level (200-400 tokens)
- **Precise data questions**: Sentence level
- **Keyword triggers**: "exact", "specific", "table" trigger precision mode
- **Preserve structure**: Never break document hierarchies

#### Q: Should I optimize for elegance or practicality?

**A**: Practicality wins every time:
- Simple keyword matching > LLM extraction
- Regex patterns > AI parsing  
- Rule-based approaches for specific tasks
- Hybrid systems over pure semantic search
- Separate approaches per industry vs universal solutions
- "Way less elegant but actually works"

---

## Resources & References

### Getting Started

1. **Repository**: [awesome-enterprise-rag](https://github.com/ambicuity/awesome-enterprise-rag)
2. **Author Contact**: riteshrana36@gmail.com
3. **Website**: [riteshrana.engineer](https://riteshrana.engineer)

### Recommended Tools

#### Model Serving
- **Ollama**: Simple local model deployment
- **vLLM**: High-performance inference server

#### Models
- **Qwen QWQ-32B**: Primary recommendation for enterprise
- **7B-13B Models**: Classification and routing (Llama, Mistral families)
- **Embedding Models**: Domain-specific fine-tuned models

#### Development Stack
- **Vector Databases**: Choose based on scale and requirements
- **Document Processing**: PyPDF2, pdfplumber for PDFs
- **Table Detection**: Custom heuristics, visual models

### Further Learning

This guide represents real-world experience from 10+ client deployments in:
- Pharmaceutical companies
- Financial institutions  
- Law firms
- Consulting firms

The challenges and solutions documented here are battle-tested in production environments handling:
- 10K-50K+ documents
- Thousands of daily queries
- Concurrent user loads
- Regulatory compliance requirements

### Contributing

This is a living document based on ongoing learning. The field evolves rapidly, and new challenges emerge with each deployment.

**Feedback Welcome**: If you're implementing enterprise RAG systems and encounter new challenges or solutions, contributions are welcome.

### Final Thoughts

**The Reality**:
- Enterprise RAG is harder than tutorials make it seem
- Edge cases with enterprise documents will test your patience
- Success is 90% engineering, 10% ML
- Most failures are process and infrastructure, not model quality
- The demand is substantial and growing

**The Reward**:
- When it works, ROI is impressive
- Teams cut search time from hours to minutes
- Critical information becomes accessible
- Organizations unlock value from decades of documents

**The Approach**:
- Start with document quality assessment
- Invest in metadata architecture
- Build hybrid search systems
- Right-size models for tasks
- Prioritize reliability over sophistication

---

**Built with real-world experience. Tested in production. Proven at scale.**

*For questions, consulting, or collaboration: riteshrana36@gmail.com*